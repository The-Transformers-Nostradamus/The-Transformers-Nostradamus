{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6toho7uRO7v"
      },
      "source": [
        "# Question 1\n",
        "## Developing an Artificial Neural Network from Scratch.\n",
        "\n",
        "In this notebook, we will be developing a feedforward neural network.\n",
        "\n",
        "We will import the MNIST dataset from keras datsets. The MNIST dataset contains images of 28x28 pixels each having values ranging from 0-255.\n",
        "It has 60000 images in the training set and 10000 images in the test set. However, we will only use the first 10000 images for training and first 1000 images for testing because our code isn't optimized and it takes time to run. We are not looking for accuracy of our network right now, we will be doing that in the next week when we will be implementing the same using Tensorflow.\n",
        "\n",
        "\n",
        "Run the first 3 cells. Your code begins after that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nI17X78rktdA"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from keras.datasets import mnist\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OrINntzulT4M",
        "outputId": "72879853-cec4-4d1e-dd27-245e969cd0b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 28, 28)\n",
            "(60000,)\n",
            "(10000, 28, 28)\n",
            "(10000,)\n"
          ]
        }
      ],
      "source": [
        "(train_X, train_y), (test_X, test_y) = mnist.load_data()\n",
        "print(train_X.shape)\n",
        "print(train_y.shape)\n",
        "print(test_X.shape)\n",
        "print(test_y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As discussed in the class, the images are flattened to a column.\n",
        "\n",
        "Then we are normalizing them by dividing by 255."
      ],
      "metadata": {
        "id": "dr4rLzb9ZBQE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0jy7CLWCEwfn"
      },
      "outputs": [],
      "source": [
        "train_X=train_X.reshape(60000,784,1)    # flattening\n",
        "test_X=test_X.reshape(10000,784,1)\n",
        "\n",
        "train_y=train_y.reshape(60000,1)\n",
        "test_y=test_y.reshape(10000,1)\n",
        "\n",
        "train_X= train_X/255\n",
        "test_X = test_X/255\n",
        "\n",
        "train_X=train_X[:10000]         #taking the first 10000 images.\n",
        "train_y=train_y[:10000]\n",
        "test_X=test_X[:1000]\n",
        "test_y=test_y[:1000]\n",
        "train_data=list(zip(train_X,train_y))\n",
        "test_data=list(zip(test_X,test_y))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Write the code for Sigmoid Function."
      ],
      "metadata": {
        "id": "wWwDzh6kZOy3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Q5a8tGYku-7"
      },
      "outputs": [],
      "source": [
        "def sigmoid(z):\n",
        "  return \"\"\"....\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 The Network\n",
        "\n",
        "We will making a class called Network which has certain functions inside it. The cost function used is Cross-Entropy Loss. You need to code only the first 3. Rest are done for you.  There are various places within the code marked as stop_zone. Read the instructions below the code at those places to check whether your code till there is correct or not."
      ],
      "metadata": {
        "id": "cIJI5SoxbJaq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RwsydmyTEt0z"
      },
      "outputs": [],
      "source": [
        "class Network(object):\n",
        "    def __init__(self,sizes): # sizes is a list containing the network. \n",
        "                              # eg : [784,128,10] means input =784 neurons,\n",
        "                              #    1st hidden layer 128 neurons, output 10 neurons.\n",
        "        self.sizes=sizes\n",
        "        self.num_layers=\"...can you say the number of layers based on the list called sizes...\"\n",
        "        self.weights= [np.random.randn(x,y) for x,y in zip(sizes[1:],sizes[:-1])]\n",
        "        self.biases= \"...can you do this by understanding the self.weights...\"\n",
        "\n",
        "# stop_zone 1. Comment out all the code below. Select all rows below. Click Ctrl + /.\n",
        "# Include the show function given below above this comment area inside the class.\n",
        "# Run this cell and then run the code with stop_zone 1 written below.\n",
        "# After this testing, don't forget tto remove the comments. Same, select all, Ctrl+/.\n",
        "\n",
        "    def forwardpropagation(self,a):\n",
        "        for b,w in zip(self.biases, self.weights):\n",
        "            a=\"...a is activation...\" # sig (w.a +b)\n",
        "            # print(a.shape)\n",
        "        return a\n",
        "\n",
        "# stop_zone 2. Comment out all the code below. Don't comment out the __init__ method else you will get error.\n",
        "# Remove comment from print(a.shape) line above. Run this cell. And run the code with stop_zone 2 written below.\n",
        "\n",
        "\n",
        "    def backpropagation(self,x,y):\n",
        "        \n",
        "        # nothing to do in this 3 lines.\n",
        "        y_t = np.zeros((len(y), 10))\n",
        "        y_t[np.arange(len(y)), y] = 1\n",
        "        y_t= y_t.T\n",
        "\n",
        "        #nabla_b=dC/db and nabla_w=dC/dw. They are lists of shapes equal to that of bias and weights.\n",
        "        nabla_b=[np.zeros(b.shape) for b in self.biases]\n",
        "        nabla_w=[np.zeros(w.shape) for w in self.weights]\n",
        "\n",
        "        # initially, a0 = input.\n",
        "        activation=x\n",
        "        activation_list=[x]\n",
        "\n",
        "        # step 1 : calculation of delta in last layer\n",
        "\n",
        "        # write the same forward propagation code here but while doing so store the a's.\n",
        "        for w,b in zip(self.weights,self.biases):\n",
        "            activation= \"...just written above...\"\n",
        "            activation_list.append(activation)\n",
        "\n",
        "        delta= \"...delta is dC/dz3... how is it calculated?...\"\n",
        "\n",
        "        # step 2 : nabla_b and nabla_w relation with delta of last layer\n",
        "\n",
        "        nabla_b[-1]=\"...how is dC/db3 and dC/dz3 related...\"\n",
        "        nabla_w[-1]= \"...how is dC/dw3 and dC/dz3 related...\"\n",
        "\n",
        "        # print(\"{} {}\".format(nabla_b[-1].shape,nabla_w[-1].shape) )\n",
        "#stop_zone 3 : remove comment from the print statement just above and run the cell for stop_zone3.\n",
        "# don't forget commenting out.\n",
        "\n",
        "        # step 3 : calculation of delta for hidden layers\n",
        "        \n",
        "        for j in range(2,self.num_layers):\n",
        "            sig_der = activation_list[-j]*(1-activation_list[-j])\n",
        "            delta= \"...how is dC/dz2 and dC/dz3 related ? Look i have calculated one term already for you (sig_der)...\"\n",
        "\n",
        "            # step 4 : nabla_b and nabla_w relation with delta of others layers\n",
        "            nabla_b[-j]= \"...again, how is dC/db2 and dC/dz2 related...\"\n",
        "            nabla_w[-j]= \"...how is dC/dw2 and dC/dz2 related...\"\n",
        "        \n",
        "#stop_zone 4 : Run the cell for stop_zone 4.\n",
        "        return (nabla_b,nabla_w)\n",
        "\n",
        "    # the functions below are complete. If you are fine till stop_zone 4, you can run\n",
        "    # this whole cell and train, test the data by running the last cell of the question.\n",
        "    # You may need to wait for around 10 minutes to see the test predictions.\n",
        "    def update_mini_batch(self,mini_batch,lr):\n",
        "        nabla_b=[np.zeros(b.shape) for b in self.biases]\n",
        "        nabla_w=[np.zeros(w.shape) for w in self.weights]\n",
        "        for x,y in mini_batch:\n",
        "            delta_b,delta_w= self.backpropagation(x,y)\n",
        "            nabla_b=[nb+ db for nb,db in zip (nabla_b,delta_b)]\n",
        "            nabla_w=[nw+dw for nw,dw in zip(nabla_w,delta_w)]\n",
        "\n",
        "        self.weights=[w- lr*nw/len(mini_batch) for w,nw in zip(self.weights,nabla_w)]\n",
        "        self.biases=[b-lr*nb/len(mini_batch) for b,nb in zip(self.biases,nabla_b)]\n",
        "\n",
        "\n",
        "    def SGD(self, train_data,epochs,mini_batch_size, lr):\n",
        "        n_train= len(train_data)\n",
        "        for i in range(epochs):\n",
        "            random.shuffle(train_data)\n",
        "            mini_batches = [train_data[k:k+ mini_batch_size] for k in range(0,n_train,mini_batch_size)]\n",
        "            for mini_batch in mini_batches:\n",
        "                self.update_mini_batch(mini_batch,lr)\n",
        "\n",
        "            self.predict(train_data)\n",
        "            print(\"Epoch {0} completed.\".format(i+1))\n",
        " \n",
        "    def predict(self,test_data):\n",
        "        test_results = [(np.argmax(self.forwardpropagation(x)),y) for x,y in test_data]\n",
        "        # returns the index of that output neuron which has highest activation\n",
        "\n",
        "        num= sum(int (x==y) for x,y in test_results)\n",
        "        print (\"{0}/{1} classified correctly.\".format(num,len(test_data)))\n",
        "         \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8u8cVnGamVgP"
      },
      "outputs": [],
      "source": [
        "# stop_zone 1\n",
        "\n",
        "def show(self):\n",
        "  print(self.num_layers)\n",
        "  for bias in self.biases:\n",
        "      print(bias.shape)\n",
        "  for weight in self.weights:\n",
        "      print(weight.shape)\n",
        "\n",
        "# Copy this show function from here. Paste it inside that Network Class.\n",
        "# Comment out the show function here. Run this cell.\n",
        "\n",
        "net=Network([784,128,64,10])\n",
        "net.show()\n",
        "\n",
        "# The desired output is :\n",
        "# 4\n",
        "# (128, 1)\n",
        "# (64, 1)\n",
        "# (10, 1)\n",
        "# (128, 784)\n",
        "# (64, 128)\n",
        "# (10, 64)\n",
        "#  If you are getting this, you are correct. Proceed to forwardpropagation.\n",
        "\n",
        "# Keeping the show function over there in the Network class doesn't make any \n",
        "# difference. You may delete it if you wish. Better toss a coin."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D7EJBF7XsSft"
      },
      "outputs": [],
      "source": [
        "# stop_zone 2\n",
        "# to use this, make sure your data is loaded. Run this cell.\n",
        "net=Network([784,128,64,10])\n",
        "print(train_X[0])\n",
        "net.forwardpropagation(train_X[0])\n",
        "\n",
        "# The desired output is :\n",
        "# (784, 1)\n",
        "# (128, 1)\n",
        "# (64, 1)\n",
        "# (10, 1)\n",
        "#  If you are getting this, you are correct. Proceed to forwardpropagation."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# stop_zone 3\n",
        "net=Network([784,128,64,10])\n",
        "net.backpropagation(train_X[0],train_y[0])\n",
        "\n",
        "# Desired output : (10,1) (10,64)"
      ],
      "metadata": {
        "id": "FwHWyaKNhIIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net=Network([784,128,64,10])\n",
        "nabla_b,nabla_w=net.backpropagation(train_X[0],train_y[0])\n",
        "for nb in nabla_b:\n",
        "  print(nb.shape)\n",
        "for nw in nabla_w:\n",
        "  print(nw.shape)\n",
        "\n",
        "# Desired output: \n",
        "# (128, 1)\n",
        "# (64, 1)\n",
        "# (10, 1)\n",
        "# (128, 784)\n",
        "# (64, 128)\n",
        "# (10, 64)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9pq4E3rHik-f",
        "outputId": "bcbbddf7-f9fb-4685-ec55-4acddb6796cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(128, 1)\n",
            "(64, 1)\n",
            "(10, 1)\n",
            "(128, 784)\n",
            "(64, 128)\n",
            "(10, 64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qXljiAYRlvdq",
        "outputId": "cddb5fe4-e61d-4df0-9587-f1d40db98562"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1953/10000 classified correctly.\n",
            "Epoch 0 completed, training accuracy None.\n",
            "3242/10000 classified correctly.\n",
            "Epoch 1 completed, training accuracy None.\n",
            "4162/10000 classified correctly.\n",
            "Epoch 2 completed, training accuracy None.\n",
            "4848/10000 classified correctly.\n",
            "Epoch 3 completed, training accuracy None.\n",
            "5361/10000 classified correctly.\n",
            "Epoch 4 completed, training accuracy None.\n",
            "5738/10000 classified correctly.\n",
            "Epoch 5 completed, training accuracy None.\n",
            "6065/10000 classified correctly.\n",
            "Epoch 6 completed, training accuracy None.\n",
            "6346/10000 classified correctly.\n",
            "Epoch 7 completed, training accuracy None.\n",
            "6570/10000 classified correctly.\n",
            "Epoch 8 completed, training accuracy None.\n",
            "6757/10000 classified correctly.\n",
            "Epoch 9 completed, training accuracy None.\n",
            "618/1000 classified correctly.\n"
          ]
        }
      ],
      "source": [
        "net=Network([784,128,64,10])\n",
        "net.SGD(train_data=train_data,epochs=10,mini_batch_size=20,lr=0.01)\n",
        "print(\"Test data:\")\n",
        "net.predict(test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# End of question 1."
      ],
      "metadata": {
        "id": "mhMIoFT9m7OU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 2 : \n",
        "## Stochastic Gradient Descent\n",
        "Implement logistic regression using \"Stochastic gradient descent\" and use iris-dataset as training data.\n",
        "\n",
        "\n",
        "The word 'stochastic' means a system or process linked with a random probability. Hence, in Stochastic Gradient Descent, a few samples are selected randomly instead of the whole data set for each iteration. In Gradient Descent, there is a term called “batch” which denotes the total number of samples from a dataset that is used for calculating the gradient for each iteration. In typical Gradient Descent optimization, like Batch Gradient Descent, the batch is taken to be the whole dataset. Although using the whole dataset is really useful for getting to the minima in a less noisy and less random manner, the problem arises when our dataset gets big. \n",
        "Suppose, you have a million samples in your dataset, so if you use a typical Gradient Descent optimization technique, you will have to use all of the one million samples for completing one iteration while performing the Gradient Descent, and it has to be done for every iteration until the minima are reached. Hence, it becomes computationally very expensive to perform.\n",
        "This problem is solved by Stochastic Gradient Descent. In SGD, it uses only a single sample, i.e., a batch size of one, to perform each iteration. The sample is randomly shuffled and selected for performing the iteration.\n",
        "\n",
        "    Stochastic Gradient Descent (SGD) is a variant of the Gradient Descent algorithm used for optimizing machine learning models. In this variant, only one random training example is used to calculate the gradient and update the parameters at each iteration. Here are some of the advantages and disadvantages of using SGD:\n",
        "\n",
        "Advantages:\n",
        "\n",
        "Speed: SGD is faster than other variants of Gradient Descent such as Batch Gradient Descent and Mini-Batch Gradient Descent since it uses only one example to update the parameters.\n",
        "\n",
        "Memory Efficiency: Since SGD updates the parameters for each training example one at a time, it is memory-efficient and can handle large datasets that cannot fit into memory.\n",
        "\n",
        "Avoidance of Local Minima: Due to the noisy updates in SGD, it has the ability to escape from local minima and converge to a global minimum.\n",
        "\n",
        "Disadvantages:\n",
        "\n",
        "Noisy updates: The updates in SGD are noisy and have a high variance, which can make the optimization process less stable and lead to oscillations around the minimum.\n",
        "\n",
        "Slow Convergence: SGD may require more iterations to converge to the minimum since it updates the parameters for each training example one at a time.\n",
        "\n",
        "Sensitivity to Learning Rate: The choice of learning rate can be critical in SGD since using a high learning rate can cause the algorithm to overshoot the minimum, while a low learning rate can make the algorithm converge slowly.\n",
        "\n",
        "Less Accurate: Due to the noisy updates, SGD may not converge to the exact global minimum and can result in a suboptimal solution. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "So, in SGD, we find out the gradient of the cost function of a single example at each iteration instead of the sum of the gradient of the cost function of all the examples.\n",
        "\n",
        "In SGD, since only one sample from the dataset is chosen at random for each iteration, the path taken by the algorithm to reach the minima is usually noisier than your typical Gradient Descent algorithm. But that doesn’t matter all that much because the path taken by the algorithm does not matter, as long as we reach the minima and with a significantly shorter training time."
      ],
      "metadata": {
        "id": "Aa_iPRK6nEay"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import math\n",
        "import random\n",
        "\n",
        "# pre load sklearn iris datasets\n",
        "from sklearn import datasets\n",
        "iris = '...'\n",
        "\n",
        "X = iris.data  \n",
        "Y = iris.target\n",
        "\n",
        "dataset = []\n",
        "\n",
        "target_label = 0 # choose the target label of flower type\n",
        "for index, x in enumerate(X):\n",
        "    transform_label = None\n",
        "    if:\n",
        "      '...'\n",
        "    else:\n",
        "        '...'\n",
        "    x = [x[0], x[2]]\n",
        "    dataset.append((x,transform_label))\n",
        "    \n",
        "dataset = np.array(dataset)\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def sgd(dataset, w):\n",
        "    #run sgd randomly\n",
        "    index = random.randint(0, len(dataset) - 1)\n",
        "    '...'\n",
        "\n",
        "def cost(dataset, w):\n",
        "    total_cost = 0\n",
        "    for x,y in dataset:\n",
        "       '...' \n",
        "    return '...'\n",
        "\n",
        "def logistic_regression(dataset):\n",
        "    w = np.zeros(2)\n",
        "    limit = 1500 #update times\n",
        "    eta = 0.1 #update rate\n",
        "    costs = []\n",
        "    for i in range(limit):\n",
        "        '...'\n",
        "        eta = eta * 0.98 #decrease update rate\n",
        "    plt.plot(range(limit), costs)\n",
        "    plt.show()\n",
        "    return w,(limit, costs)\n",
        "\n",
        "def main():\n",
        "    #execute\n",
        "    w = logistic_regression(dataset)\n",
        "    #draw \n",
        "    ps = [v[0] for v in dataset]\n",
        "    label = [v[1] for v in dataset]\n",
        "    fig = plt.figure()\n",
        "    ax1 = fig.add_subplot(111)\n",
        "    #plot via label\n",
        "    tpx=[]\n",
        "    for index, label_value in enumerate(label):\n",
        "        px=ps[index][0]\n",
        "        py=ps[index][1]\n",
        "        tpx.append(px)\n",
        "        if label_value == 1:\n",
        "            ax1.scatter(px, py, c='b', marker=\"o\", label='O')\n",
        "        else:\n",
        "            ax1.scatter(px, py, c='r', marker=\"x\", label='X')\n",
        "\n",
        "    l = np.linspace(min(tpx),max(tpx))\n",
        "    a,b = (-w[0][0]/w[0][1], w[0][0])\n",
        "    ax1.plot(l, a*l + b, 'g-')\n",
        "    #plt.legend(loc='upper left');\n",
        "    plt.show()\n",
        "\n",
        "    limit = w[1][0]\n",
        "    costs = w[1][1]\n",
        "    w = w[0]\n",
        "\n",
        "    # calculate score\n",
        "    predicted_Y=[]\n",
        "    answer_Y=[]\n",
        "    for X,Y in dataset:\n",
        "        '...'\n",
        "    predicted_Y = np.asarray(predicted_Y)\n",
        "    predicted_Y = predicted_Y > 0.5\n",
        "    print(answer_Y)\n",
        "    print(predicted_Y)\n",
        "    \n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "RMgp1wALns9d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 3 :\n",
        "## Implement linear regression using \"mini-batch\" gradient descent\n",
        "\n",
        "\n",
        "Mini-Batch Gradient Descent: Parameters are updated after computing the gradient of  the error with respect to a subset of the training set.\n",
        "Thus, mini-batch gradient descent makes a compromise between the speedy convergence and the noise associated with gradient update which makes it a more flexible and robust algorithm.\n",
        "\n",
        "\n",
        " Mini-Batch Gradient Descent: Algorithm-\n",
        "\n",
        "    Let theta = model parameters and max_iters = number of epochs. for itr = 1, 2, 3, …, max_iters:       for mini_batch (X_mini, y_mini):\n",
        "\n",
        "        Forward Pass on the batch X_mini:\n",
        "            Make predictions on the mini-batch\n",
        "            Compute error in predictions (J(theta)) with the current values of the parameters\n",
        "        Backward Pass:\n",
        "            Compute gradient(theta) = partial derivative of J(theta) w.r.t. theta\n",
        "        Update parameters:\n",
        "            theta = theta – learning_rate*gradient(theta)"
      ],
      "metadata": {
        "id": "I1CbSYwCn0BN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# creating data\n",
        "mean = np.array([5.0, 6.0])\n",
        "cov = np.array([[1.0, 0.95], [0.95, 1.2]])\n",
        "data = np.random.multivariate_normal(mean, cov, 8000)\n",
        "\n",
        "# visualising data\n",
        "plt.scatter(data[:500, 0], data[:500, 1], marker='.')\n",
        "plt.show()\n",
        "\n",
        "# train-test-split\n",
        "data = np.hstack((np.ones((data.shape[0], 1)), data))\n",
        "\n",
        "split_factor = 0.90\n",
        "split = int(split_factor * data.shape[0])\n",
        "\n",
        "X_train = data[:split, :-1]\n",
        "y_train = data[:split, -1].reshape((-1, 1))\n",
        "X_test = data[split:, :-1]\n",
        "y_test = data[split:, -1].reshape((-1, 1))\n",
        "\n",
        "\n",
        "\n",
        "# linear regression using \"mini-batch\" gradient descent\n",
        "# function to compute hypothesis / predictions\n",
        "\n",
        "\n",
        "def hypothesis(X, theta):\n",
        "\treturn '...'\n",
        "\n",
        "# function to compute gradient of error function w.r.t. theta\n",
        "\n",
        "\n",
        "def gradient(X, y, theta):\n",
        "\t'...'\n",
        "\n",
        "# function to compute the error for current values of theta\n",
        "\n",
        "\n",
        "def cost(X, y, theta):\n",
        "\t'...'\n",
        "\n",
        "# function to create a list containing mini-batches\n",
        "\n",
        "\n",
        "def create_mini_batches(X, y, batch_size):\n",
        "\tmini_batches = []\n",
        "\tdata = np.hstack((X, y))\n",
        "\tnp.random.shuffle(data)\n",
        "\tn_minibatches = data.shape[0] // batch_size\n",
        "\ti = 0\n",
        "\n",
        "\tfor i in range(n_minibatches + 1):\n",
        "\t\t'...'\n",
        "\tif data.shape[0] % batch_size != 0:\n",
        "\t\t'...'\n",
        "\treturn '...'\n",
        "\n",
        "# function to perform mini-batch gradient descent\n",
        "\n",
        "\n",
        "def gradientDescent(X, y, learning_rate=0.001, batch_size=32):\n",
        "\t'...'\n",
        "\n",
        "\n",
        "theta, error_list = gradientDescent(X_train, y_train)\n",
        "print(\"Bias = \", theta[0])\n",
        "print(\"Coefficients = \", theta[1:])\n",
        "\n",
        "# visualising gradient descent\n",
        "plt.plot(error_list)\n",
        "plt.xlabel(\"Number of iterations\")\n",
        "plt.ylabel(\"Cost\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# predicting output for X_test\n",
        "y_pred = hypothesis(X_test, theta)\n",
        "plt.scatter(X_test[:, 1], y_test[:, ], marker='.')\n",
        "plt.plot(X_test[:, 1], y_pred, color='orange')\n",
        "plt.show()\n",
        "\n",
        "# calculating error in predictions\n",
        "error = '...'\n",
        "\n"
      ],
      "metadata": {
        "id": "Oz46YIVYseN0"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}